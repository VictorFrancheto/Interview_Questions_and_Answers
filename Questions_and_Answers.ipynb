{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a172100",
   "metadata": {},
   "source": [
    "**Question 1:** Write a Python function to implement logistic regression using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac1654d",
   "metadata": {},
   "source": [
    "**Answer:** Let $\\phi(z)$ be the logistic function, defined as\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "This function maps any real value $z$ into the interval $[0, 1]$, making it widely used in binary classification problems.\n",
    "\n",
    "### Gradient Implementation\n",
    "\n",
    "The logistic regression model aims to find the parameters $\\theta$ that minimize the error between the predictions $\\phi(z)$ and the actual values $y$. This is achieved through gradient descent.\n",
    "\n",
    "The parameter vector $\\theta$ is updated iteratively using the formula\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\alpha$ is the learning rate, a scalar value that controls the step size in the gradient direction.\n",
    "- $\\nabla J(\\theta)$ is the gradient of the cost function, measuring the slope of the error relative to $\\theta$.\n",
    "\n",
    "### Gradient of Logistic Regression\n",
    "\n",
    "The cost of logistic regression is measured using the log-likelihood loss function, and the parameter updates are based on its derivative with respect to $\\theta$\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\frac{1}{m} X^T (\\phi(X\\theta) - y)\n",
    "$$\n",
    "\n",
    "where\n",
    "- $m$ is the number of examples in the dataset.\n",
    "- $X$ is the feature matrix with dimensions $(m \\times n)$, where $n$ is the number of features (including the intercept term).\n",
    "- $y$ is the vector of actual labels.\n",
    "- $\\phi(X\\theta)$ are the model's predictions for all examples.\n",
    "\n",
    "### Parameter Update Steps\n",
    "\n",
    "For each iteration, the steps are as follows:\n",
    "1. Compute the product $z = X\\theta$, where $z$ represents the predicted values before applying the logistic function.\n",
    "2. Obtain the predictions $\\phi(z)$ by applying the logistic function to $z$:\n",
    "   $$\n",
    "   \\phi(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "3. Calculate the error as the difference between predictions and actual values:\n",
    "   $$\n",
    "   \\text{error} = \\phi(z) - y\n",
    "   $$\n",
    "4. Compute the gradient:\n",
    "   $$\n",
    "   \\nabla J(\\theta) = \\frac{1}{m} X^T (\\phi(z) - y)\n",
    "   $$\n",
    "5. Update the parameters $\\theta$:\n",
    "   $$\n",
    "   \\theta := \\theta - \\alpha \\nabla J(\\theta)\n",
    "   $$\n",
    "\n",
    "### Considerations about $\\alpha$\n",
    "The learning rate $\\alpha$ must be chosen carefully. Large values may prevent convergence, while small values can slow down the process.\n",
    "\n",
    "### Final Result\n",
    "After several iterations, the parameters $\\theta$ converge to values that minimize the cost, allowing the model to make accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa5c4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_regression(X, y, learning_rate=0.01, iterations=1000):\n",
    "    m, n = X.shape\n",
    "    X = np.insert(X, 0, 1, axis=1)  # Add intercept term\n",
    "    theta = np.zeros(n + 1)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        z = X.dot(theta)\n",
    "        predictions = sigmoid(z)\n",
    "        errors = predictions - y\n",
    "        gradient = X.T.dot(errors) / m\n",
    "        theta -= learning_rate * gradient\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b09af",
   "metadata": {},
   "source": [
    "**Question 2:** What is the purpose of feature selection in machine learning?\n",
    "\n",
    "**Answer:** Feature selection is the process of identifying and selecting the most important variables (features) that contribute to the model’s predictive power. This helps in improving model performance, reducing overfitting, and decreasing training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a287d67",
   "metadata": {},
   "source": [
    "**Question 3:** Assume you need to generate a predictive model using multiple regression. Explain how you intend to validate this model.\n",
    "\n",
    "Answer: To validate a multiple regression model, I would focus on these two methods:\n",
    "\n",
    "1. **Train-Test Split:**\n",
    "   - Split the data into training (e.g., 80%) and test (e.g., 20%) sets.\n",
    "   - Train the model on the training data and evaluate its performance on the test data using metrics like \\(R^2\\), RMSE, or MAE to assess predictive accuracy.\n",
    "\n",
    "2. **Residual Analysis:**\n",
    "   - Examine residual plots to verify assumptions of linearity, homoscedasticity (constant variance of residuals), and independence.\n",
    "   - Check for patterns or systematic deviations, which might indicate issues with the model's fit.\n",
    "\n",
    "This combination ensures both predictive validity and the assessment of fundamental assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f24fa5",
   "metadata": {},
   "source": [
    "## Questions about Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771fa338",
   "metadata": {},
   "source": [
    "**Question 1:** What are recurrent neural networks (RNN)? \n",
    "\n",
    "**Answer:** Recurrent neural networks, also known as RNNs, are a class of neural networks that allow previous outputs to be used as inputs while having hidden states.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df227385",
   "metadata": {},
   "source": [
    "**Question 2:** What is the role of the activation function?\n",
    "\n",
    "**Answer:** The purpose of the activation function is to introduce non-linearity into the output of a neuron. The activation function decides whether a neuron should be activated or not by calculating weighted sum and further adding bias with it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99103a3e",
   "metadata": {},
   "source": [
    "**Question 3:** What happens if the learning rate is set too high or too low?\n",
    "\n",
    "**Answer:** If the learning rate is too low, your model will train very slowly as minimal updates are made to the weights through each iteration. Thus, it would take many updates before reaching the minimum point.\n",
    "If the learning rate is set too high, this causes undesirable divergent behavior to the loss function due to drastic updates in weights, and it may fail to converge.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59bce8d",
   "metadata": {},
   "source": [
    "## Questions about LLM's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f9618",
   "metadata": {},
   "source": [
    "**Question 1:** What is Tokenization?\n",
    "\n",
    "**Answer:** Tokenization is the process of breaking text into smaller units, called **tokens**, for natural language processing (NLP). Tokens can be words, subwords, characters, or sentences. \n",
    "\n",
    "### Types:\n",
    "1. **Word Tokenization:** Splits text into words (e.g., `\"I love NLP!\" → [\"I\", \"love\", \"NLP\", \"!\"]`).\n",
    "2. **Subword Tokenization:** Breaks words into subunits (e.g., `\"unbelievable\" → [\"un\", \"believ\", \"able\"]`).\n",
    "3. **Character Tokenization:** Splits text into characters.\n",
    "4. **Sentence Tokenization:** Splits text into sentences.\n",
    "\n",
    "### Importance:\n",
    "- Prepares text for models.\n",
    "- Handles multilingual data.\n",
    "- Maps tokens to numerical representations.\n",
    "\n",
    "### Applications:\n",
    "Used in tasks like sentiment analysis, machine translation, and text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f24570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'NLP', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\victor.francheto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I love NLP!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a4154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "text = \"I love NLP!\"\n",
    "tokens = simple_preprocess(text)\n",
    "print(tokens) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6498d1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'nlp': 3}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([\"I love NLP!\"])\n",
    "print(tokenizer.word_index)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d978d4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55d5fc",
   "metadata": {},
   "source": [
    "**Question 2:** What is Normalization.\n",
    "\n",
    "**Answer:** Preprocess the text by lowercasing, removing unnecessary spaces, and handling special characters to standardize the input.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd2049",
   "metadata": {},
   "source": [
    " **Question:** What is the role of embeddings in LLM's?\n",
    " \n",
    " **Answer:** Embeddings in LLMs are numerical representations that convert words or tokens into dense vectors in a high-dimensional continuous space. These vectors capture the meaning of words and their context, enabling the model to understand and generate language more effectively.\n",
    " \n",
    " ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
